{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load csv to dataframe\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create split date\n",
    "\n",
    "split = '1/12/2017'\n",
    "split = pd.to_datetime(split, format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on row 520000\n"
     ]
    }
   ],
   "source": [
    "## Missing Values\n",
    "# cloud coverage: 0 if no events, 8 if events\n",
    "for row in range(len(df)):\n",
    "    if row % 10000 == 0:\n",
    "        clear_output()\n",
    "        print(\"Working on row {}\".format(row))\n",
    "    if np.isnan(df.loc[row, 'CloudCover']):\n",
    "        if df.loc[row, 'Events'] is np.nan:\n",
    "            df.loc[row, 'CloudCover'] = 0\n",
    "        else:\n",
    "            df.loc[row, 'CloudCover'] = 8\n",
    "\n",
    "# max gust speed = max wind speed\n",
    "df.Max_Gust_SpeedKm_h = df.Max_Gust_SpeedKm_h.fillna(df.Max_Wind_SpeedKm_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date Features\n",
    "# convert date to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# add features\n",
    "# df['DayN']=df['Date'].dt.dayofyear    # non credo possa servire\n",
    "df['DayOfWeek']=df['Date'].dt.dayofweek\n",
    "df['Month']=df['Date'].dt.month\n",
    "df['Week']=df['Date'].dt.weekofyear\n",
    "df['Quarter']=df['Date'].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1748\n"
     ]
    }
   ],
   "source": [
    "## Temporal Features\n",
    "# create columns\n",
    "df['IsOpen_yesterday'] = np.empty(len(df))\n",
    "df['IsOpen_tomorrow'] = np.empty(len(df))\n",
    "df['IsHoliday_yesterday'] = np.empty(len(df))\n",
    "df['IsHoliday_tomorrow'] = np.empty(len(df))\n",
    "df['NumberOfSales_yesterday'] = np.empty(len(df))\n",
    "df['NumberOfSales_lastweek'] = np.empty(len(df))\n",
    "df['NumberOfSales_lastmonth'] = np.empty(len(df))\n",
    "\n",
    "for store in df.StoreID.unique():\n",
    "    clear_output()\n",
    "    print(\"Working on {}\".format(store))\n",
    "    temp = df.loc[df.StoreID == store]\n",
    "    # switch index to timestamps to make this easier\n",
    "    oldindex = temp.index\n",
    "    temp.index = temp['Date']\n",
    "    \n",
    "    temp['IsOpen_yesterday'] = temp.IsOpen.rolling(window='1d',closed='left', min_periods=1).sum()\n",
    "    temp['IsOpen_tomorrow'] = temp.IsOpen.rolling(window='1d',closed='left', min_periods=1).sum().shift(-2, '1d')\n",
    "    temp['IsHoliday_yesterday'] = temp.IsHoliday.rolling(window='1d',closed='left', min_periods=1).sum()\n",
    "    temp['IsHoliday_tomorrow'] = temp.IsHoliday.rolling(window='1d',closed='left', min_periods=1).sum().shift(-2, '1d')\n",
    "    temp['NumberOfSales_yesterday'] = temp.NumberOfSales.rolling(window='1d',closed='left', min_periods=1).sum()\n",
    "    temp['NumberOfSales_lastweek'] = temp.NumberOfSales.rolling(window='7d',closed='left', min_periods=1).sum()\n",
    "    temp['NumberOfSales_lastmonth'] = temp.NumberOfSales.rolling(window='30d',closed='left', min_periods=1).sum()\n",
    "    \n",
    "    # put it back in the dataframe\n",
    "    temp.index = oldindex\n",
    "    df.loc[df.StoreID == store] = temp\n",
    "    \n",
    "# Attenzione: i valori di tomorrow nel test sono sputtanati a NaN\n",
    "# vanno messi a mano qui o cambiato il modo di calcolo\n",
    "    \n",
    "# drop rows at the beginning where we have no past information\n",
    "# NB: possiamo fare a meno se togliamo quelle feature\n",
    "df = df.iloc[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "# df = df.drop('Date', axis=1)\n",
    "df = df.drop('NumberOfCustomers', axis=1)  \n",
    "df = df.drop('WindDirDegrees', axis=1)\n",
    "\n",
    "# questi non cambiano mai, teniamo regione e population\n",
    "df = df.drop('Region_AreaKM2', axis=1)\n",
    "df = df.drop('Region_GDP', axis=1)\n",
    "#df = df.drop('Region_PopulationK', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with IsOpen = 0\n",
    "# -> number of sales is always = 0 \n",
    "df = df[df.IsOpen == 1]\n",
    "df = df.drop('IsOpen', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistemo i dati per regression tree\n",
    "\n",
    "# One-Hot Encoding \n",
    "# nb: pd.get_dummies rimuove le colonne direttamente\n",
    "\n",
    "# ## StoreId\n",
    "# df = pd.get_dummies(df, columns=['StoreID'], prefix='StoreID')\n",
    "\n",
    "df.drop('StoreID',axis=1)\n",
    "\n",
    " ## StoreType\n",
    "df = pd.get_dummies(df, columns=['StoreType'], prefix='StoreType')\n",
    "\n",
    " ## AssortmentType\n",
    "df = pd.get_dummies(df, columns=['AssortmentType'], prefix='AssortmentType')\n",
    "\n",
    "# ## Region\n",
    "df = pd.get_dummies(df, columns=['Region'], prefix='Region')\n",
    "\n",
    "# ## Events\n",
    "# df = pd.get_dummies(df, columns=['Events'], prefix='Events', dummy_na=True)\n",
    "### inutile se possiamo usare categorie con decision tree\n",
    "\n",
    "# numeric features to categories (strings)\n",
    "#df.StoreID = df.StoreID.astype(str)\n",
    "#df.Region = df.Region.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No-Events (NaN) are considered as sunny days, with lowest value (0) on the events scale\n",
    "df['Events'] = df['Events'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.replace({'Rain':1, 'Thunderstorm':1, 'Fog':1, 'Snow': 2, 'Fog-Rain': 2, 'Rain-Thunderstorm': 2, 'Rain-Snow':2, 'Fog-Snow':2, 'Fog-Rain-Snow':3, 'Rain-Hail':3, 'Snow-Hail':3, 'Rain-Snow-Hail':3, 'Fog-Rain-Hail':3, 'Fog-Thunderstorm':3, 'Fog-Rain-Thunderstorm':4, 'Fog-Snow-Hail':4, 'Fog-Rain-Snow-Hail':4, 'Rain-Snow-Thunderstorm':4, 'Rain-Hail-Thunderstorm':4, 'Fog-Rain-Hail-Thunderstorm':4, 'Rain-Snow-Hail-Thunderstorm':4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "df_train = df[df.Date < split]\n",
    "df_validation = df[df.Date >= split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Date\n",
    "\n",
    "df_train = df_train.drop('Date', axis=1)\n",
    "df_validation = df_validation.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split trainset in data and target\n",
    "\n",
    "y = df_train[\"NumberOfSales\"]\n",
    "X = df_train.drop('NumberOfSales', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with NaN:\n",
      "Max_VisibilityKm\n",
      "Mean_VisibilityKm\n",
      "Min_VisibilitykM\n",
      "IsOpen_yesterday\n",
      "IsOpen_tomorrow\n",
      "IsHoliday_yesterday\n",
      "IsHoliday_tomorrow\n",
      "NumberOfSales_yesterday\n",
      "NumberOfSales_lastweek\n",
      "NumberOfSales_lastmonth\n"
     ]
    }
   ],
   "source": [
    "# Looking for features with NaN values\n",
    "null_cols = []\n",
    "print('Features with NaN:')\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().values.any():\n",
    "        print(col)\n",
    "        null_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting indices (rows) of all NaN values\n",
    "inds = pd.isnull(X).any(1).nonzero()[0]\n",
    "\n",
    "# drop all the rows with NaN values\n",
    "y = y.drop(y.index[inds])\n",
    "X = X.drop(X.index[inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (370616, 55)\n",
      "y: (370616,)\n"
     ]
    }
   ],
   "source": [
    "# checking shapes\n",
    "print('X: ' + str(X.shape))\n",
    "print('y: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit random forest with 250 trees\n",
    "forest = RandomForestRegressor(n_estimators=250, random_state=0)\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split validation in data and target\n",
    "\n",
    "y_val = df_validation[\"NumberOfSales\"]\n",
    "X_val = df_validation.drop('NumberOfSales', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with NaN:\n",
      "Max_VisibilityKm\n",
      "Mean_VisibilityKm\n",
      "Min_VisibilitykM\n",
      "IsOpen_tomorrow\n",
      "IsHoliday_tomorrow\n"
     ]
    }
   ],
   "source": [
    "# Looking for features with NaN values\n",
    "null_cols = []\n",
    "print('Features with NaN:')\n",
    "for col in X_val.columns:\n",
    "    if X_val[col].isnull().values.any():\n",
    "        print(col)\n",
    "        null_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting indices (rows) of all NaN values\n",
    "inds = pd.isnull(X_val).any(1).nonzero()[0]\n",
    "\n",
    "# drop all the rows with NaN values\n",
    "y_val = y_val.drop(y_val.index[inds])\n",
    "X_val = X_val.drop(X_val.index[inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = forest.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(X_val, y_val, y_pred):\n",
    "    e_r = []\n",
    "    \n",
    "    # adjust shape\n",
    "    X_val = X_val.reset_index(drop=True)\n",
    "    y_pred = y_pred.tolist()\n",
    "    y_val = y_val.tolist()\n",
    "\n",
    "    for i in range(11):\n",
    "\n",
    "        error = 0\n",
    "        y_somma = 0\n",
    "\n",
    "        region = 'Region_' + str(i)\n",
    "        indexes = X_val.index[X_val[region] == 1].tolist()\n",
    "\n",
    "        for j in indexes:\n",
    "            \n",
    "            error += abs(y_pred[j] - y_val[j])\n",
    "            y_somma = y_somma + y_val[j]\n",
    "\n",
    "        e_r.append(error/y_somma)\n",
    "\n",
    "    return sum(e_r)/len(e_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region_0\n",
      "Region_1\n",
      "Region_2\n",
      "Region_3\n",
      "Region_4\n",
      "Region_5\n",
      "Region_6\n",
      "Region_7\n",
      "Region_8\n",
      "Region_9\n",
      "Region_10\n"
     ]
    }
   ],
   "source": [
    "evaluation = eval(X_val, y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13169826588577588"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
